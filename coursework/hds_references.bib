
@misc{noauthor_content_nodate,
	title = {Content / {High}-{Dimensional} {Statistics} (25/26)},
	url = {https://blackboard.durham.ac.uk/ultra/courses/_68465_1/outline},
	urldate = {2025-12-16},
	file = {Content / High-Dimensional Statistics (25/26):C\:\\Users\\nicsa\\Zotero\\storage\\S2YKP5RV\\outline.html:text/html},
}

@inproceedings{bai_methodologies_2008,
	address = {National University of Singapore},
	title = {{METHODOLOGIES} {IN} {SPECTRAL} {ANALYSIS} {OF} {LARGE} {DIMENSIONAL} {RANDOM} {MATRICES}, {A} {REVIEW}},
	isbn = {978-981-279-308-9 978-981-279-309-6},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812793096_0015},
	doi = {10.1142/9789812793096_0015},
	abstract = {In this paper, we give a brief review of the theory of spectral analysis of large dimensional random matrices. Most of the existing work in the literature has been stated for real matrices but the corresponding results for the complex case are also of interest, especially for researchers in Electrical and Electronic Engineering. Thus, we convert almost all results to the complex case, whenever possible. Only the latest results, including some new ones, are stated as theorems here. The main purpose of the paper is to show how important methodologies, or mathematical tools, have helped to develop the theory. Some unsolved problems are also stated.},
	language = {en},
	urldate = {2025-12-16},
	booktitle = {Advances in {Statistics}},
	publisher = {WORLD SCIENTIFIC},
	author = {Bai, Z. D.},
	month = feb,
	year = {2008},
	pages = {174--240},
	file = {PDF:C\:\\Users\\nicsa\\Zotero\\storage\\RSEKS2JU\\Bai - 2008 - METHODOLOGIES IN SPECTRAL ANALYSIS OF LARGE DIMENSIONAL RANDOM MATRICES, A REVIEW.pdf:application/pdf},
}

@article{baik_eigenvalues_2006,
	title = {Eigenvalues of large sample covariance matrices of spiked population models},
	volume = {97},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X0500134X},
	doi = {10.1016/j.jmva.2005.08.003},
	abstract = {We consider a spiked population model, proposed by Johnstone, in which all the population eigenvalues are one except for a few ﬁxed eigenvalues. The question is to determine how the sample eigenvalues depend on the non-unit population ones when both sample size and population size become large. This paper completely determines the almost sure limits of the sample eigenvalues in a spiked model for a general class of samples.},
	language = {en},
	number = {6},
	urldate = {2025-12-16},
	journal = {Journal of Multivariate Analysis},
	author = {Baik, Jinho and Silverstein, Jack W.},
	month = jul,
	year = {2006},
	pages = {1382--1408},
	file = {PDF:C\:\\Users\\nicsa\\Zotero\\storage\\MDEZ6FAH\\Baik and Silverstein - 2006 - Eigenvalues of large sample covariance matrices of spiked population models.pdf:application/pdf},
}

@article{baik_phase_2005,
	title = {Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
	volume = {33},
	issn = {0091-1798, 2168-894X},
	url = {https://projecteuclid.org/journals/annals-of-probability/volume-33/issue-5/Phase-transition-of-the-largest-eigenvalue-for-nonnull-complex-sample/10.1214/009117905000000233.full},
	doi = {10.1214/009117905000000233},
	abstract = {We compute the limiting distributions of the largest eigenvalue of a complex Gaussian sample covariance matrix when both the number of samples and the number of variables in each sample become large. When all but finitely many, say r, eigenvalues of the covariance matrix are the same, the dependence of the limiting distribution of the largest eigenvalue of the sample covariance matrix on those distinguished r eigenvalues of the covariance matrix is completely characterized in terms of an infinite sequence of new distribution functions that generalize the Tracy–Widom distributions of the random matrix theory. Especially a phase transition phenomenon is observed. Our results also apply to a last passage percolation model and a queueing model.},
	number = {5},
	urldate = {2025-12-16},
	journal = {The Annals of Probability},
	author = {Baik, Jinho and Arous, Gérard Ben and Péché, Sandrine},
	month = sep,
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {15A52, 41A60, 60F99, 62E20, 62H20, Airy kernel, limit theorem, Random matrix, Sample covariance, Tracy–Widom distribution},
	pages = {1643--1697},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\SYGCMG35\\Baik et al. - 2005 - Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices.pdf:application/pdf},
}

@book{buhlmann_statistics_2011,
	title = {Statistics for {High}-{Dimensional} {Data}: {Methods}, {Theory} and {Applications}},
	isbn = {978-3-642-20192-9},
	shorttitle = {Statistics for {High}-{Dimensional} {Data}},
	abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and versions of it for various models, boosting methods, undirected graphical modeling, and procedures controlling false positive selections.A special characteristic of the book is that it contains comprehensive mathematical theory on high-dimensional statistics combined with methodology, algorithms and illustrations with real data examples. This in-depth approach highlights the methods’ great potential and practical applicability in a variety of settings. As such, it is a valuable resource for researchers, graduate students and experts in statistics, applied mathematics and computer science.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Bühlmann, Peter and Geer, Sara van de},
	month = jun,
	year = {2011},
	note = {Google-Books-ID: S6jYXmh988UC},
	keywords = {Computers / Computer Science, Computers / Information Technology, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{chang_using_1983,
	title = {On {Using} {Principal} {Components} before {Separating} a {Mixture} of {Two} {Multivariate} {Normal} {Distributions}},
	volume = {32},
	issn = {1467-9876},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/2347949},
	doi = {10.2307/2347949},
	abstract = {In applying principal components for reducing the dimension of the data before clustering, it has ordinarily been the practice to use components with the largest eigenvalues. We prove, by means of a mixture of two multivariate normal distributions, that this practice is not justified in general. A relationship between the distance of the two sub populations and any subset of principal components is derived, showing that the components with the larger eigenvalues do not necessarily contain more information (distance). This result is further demonstrated through hypothetical as well as real situations which use actual data. The effect of scaling the variables on the distribution of the information to different components is investigated. An application to a mixture of two normal distributions is illustrated by utilizing a set of generated data in which the information is concentrated in the components with the largest and the smallest eigenvalues.},
	language = {en},
	number = {3},
	urldate = {2025-12-16},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Chang, Wei-Chien},
	year = {1983},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2347949},
	keywords = {clustering, distance, eigenvalues, mixture of two normal distributions, principal components, scaling, selection of component},
	pages = {267--275},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\WEDJZX4M\\Chang - 1983 - On Using Principal Components before Separating a Mixture of Two Multivariate Normal Distributions.pdf:application/pdf;Snapshot:C\:\\Users\\nicsa\\Zotero\\storage\\M7U7P8Z3\\2347949.html:text/html},
}

@inproceedings{ding_k_2004,
	address = {Banff, Alberta, Canada},
	title = {\textit{{K}} -means clustering via principal component analysis},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015408},
	doi = {10.1145/1015330.1015408},
	language = {en},
	urldate = {2025-12-16},
	booktitle = {Twenty-first international conference on {Machine} learning  - {ICML} '04},
	publisher = {ACM Press},
	author = {Ding, Chris and He, Xiaofeng},
	year = {2004},
	pages = {29},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\PF3D4Y8P\\Ding and He - 2004 - K -means clustering via principal component analysis.pdf:application/pdf},
}

@book{gersho_vector_2012,
	title = {Vector {Quantization} and {Signal} {Compression}},
	isbn = {978-1-4615-3626-0},
	abstract = {Herb Caen, a popular columnist for the San Francisco Chronicle, recently quoted a Voice of America press release as saying that it was reorganizing in order to "eliminate duplication and redundancy. " This quote both states a goal of data compression and illustrates its common need: the removal of duplication (or redundancy) can provide a more efficient representation of data and the quoted phrase is itself a candidate for such surgery. Not only can the number of words in the quote be reduced without losing informa tion, but the statement would actually be enhanced by such compression since it will no longer exemplify the wrong that the policy is supposed to correct. Here compression can streamline the phrase and minimize the em barassment while improving the English style. Compression in general is intended to provide efficient representations of data while preserving the essential information contained in the data. This book is devoted to the theory and practice of signal compression, i. e. , data compression applied to signals such as speech, audio, images, and video signals (excluding other data types such as financial data or general purpose computer data). The emphasis is on the conversion of analog waveforms into efficient digital representations and on the compression of digital information into the fewest possible bits. Both operations should yield the highest possible reconstruction fidelity subject to constraints on the bit rate and implementation complexity.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Gersho, Allen and Gray, Robert M.},
	month = dec,
	year = {2012},
	note = {Google-Books-ID: GgnrBwAAQBAJ},
	keywords = {Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / General, Technology \& Engineering / Signals \& Signal Processing},
}

@book{giraud_introduction_2021,
	address = {New York},
	edition = {2},
	title = {Introduction to {High}-{Dimensional} {Statistics}},
	isbn = {978-1-003-15874-5},
	abstract = {Praise for the first edition:
"[This book] succeeds singularly at providing a structured introduction to this active field of research. … it is arguably the},
	publisher = {Chapman and Hall/CRC},
	author = {Giraud, Christophe},
	month = aug,
	year = {2021},
	doi = {10.1201/9781003158745},
}

@article{hall_geometric_2005,
	title = {Geometric {Representation} of {High} {Dimension}, {Low} {Sample} {Size} {Data}},
	volume = {67},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/j.1467-9868.2005.00510.x},
	doi = {10.1111/j.1467-9868.2005.00510.x},
	abstract = {High dimension, low sample size data are emerging in various areas of science. We find a common structure underlying many such data sets by using a non-standard type of asymptotics: the dimension tends to ∞ while the sample size is fixed. Our analysis shows a tendency for the data to lie deterministically at the vertices of a regular simplex. Essentially all the randomness in the data appears only as a random rotation of this simplex. This geometric representation is used to obtain several new statistical insights.},
	number = {3},
	urldate = {2025-12-16},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Hall, Peter and Marron, J. S. and Neeman, Amnon},
	month = jun,
	year = {2005},
	pages = {427--444},
	file = {Snapshot:C\:\\Users\\nicsa\\Zotero\\storage\\LYSXRHBA\\j.1467-9868.2005.00510.html:text/html},
}

@article{hartigan_algorithm_1979,
	title = {Algorithm {AS} 136: {A} {K}-{Means} {Clustering} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	shorttitle = {Algorithm {AS} 136},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	number = {1},
	urldate = {2025-12-16},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	year = {1979},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	pages = {100--108},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\7C5H6PFC\\Hartigan and Wong - 1979 - Algorithm AS 136 A K-Means Clustering Algorithm.pdf:application/pdf},
}

@book{hastie_elements_2001,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4899-0519-2 978-0-387-21606-5},
	url = {http://link.springer.com/10.1007/978-0-387-21606-5},
	urldate = {2025-12-16},
	publisher = {Springer},
	author = {Hastie, Trevor and Friedman, Jerome and Tibshirani, Robert},
	year = {2001},
	doi = {10.1007/978-0-387-21606-5},
	keywords = {algorithms, bioinformatics, Boosting, classification, clustering, data mining, ensemble method, learning, machine learning, neural networks, Random Forest, statistics, supervised learning, Support Vector Machine, unsupervised learning},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\YHZYNVK2\\Hastie et al. - 2001 - The Elements of Statistical Learning.pdf:application/pdf},
}

@article{hebiri_smooth-lasso_2011,
	title = {The {Smooth}-{Lasso} and other ℓ1+ℓ2-penalized methods},
	volume = {5},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-5/issue-none/The-Smooth-Lasso-and-other-%e2%84%931%e2%84%932-penalized-methods/10.1214/11-EJS638.full},
	doi = {10.1214/11-EJS638},
	abstract = {We consider a linear regression problem in a high dimensional setting where the number of covariates p can be much larger than the sample size n. In such a situation, one often assumes sparsity of the regression vector, i.e., the regression vector contains many zero components. We propose a Lasso-type estimator β̂Quad (where ‘Quad’ stands for quadratic) which is based on two penalty terms. The first one is the ℓ1 norm of the regression coefficients used to exploit the sparsity of the regression as done by the Lasso estimator, whereas the second is a quadratic penalty term introduced to capture some additional information on the setting of the problem. We detail two special cases: the Elastic-Net β̂EN introduced in [42], which deals with sparse problems where correlations between variables may exist; and the Smooth-Lasso β̂SL, which responds to sparse problems where successive regression coefficients are known to vary slowly (in some situations, this can also be interpreted in terms of correlations between successive variables). From a theoretical point of view, we establish variable selection consistency results and show that β̂Quad achieves a Sparsity Inequality, i.e., a bound in terms of the number of non-zero components of the ‘true’ regression vector. These results are provided under a weaker assumption on the Gram matrix than the one used by the Lasso. In some situations this guarantees a significant improvement over the Lasso. Furthermore, a simulation study is conducted and shows that the S-Lasso β̂SL performs better than known methods as the Lasso, the Elastic-Net β̂EN, and the Fused-Lasso (introduced in [30]) with respect to the estimation accuracy. This is especially the case when the regression vector is ‘smooth’, i.e., when the variations between successive coefficients of the unknown parameter of the regression are small. The study also reveals that the theoretical calibration of the tuning parameters and the one based on 10 fold cross validation imply two S-Lasso solutions with close performance.},
	number = {none},
	urldate = {2025-12-16},
	journal = {Electronic Journal of Statistics},
	author = {Hebiri, Mohamed and Geer, Sara van de},
	month = jan,
	year = {2011},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62F12, 62H20, 62J05, 62J07, elastic-net, High-dimensional data, LARS, Lasso, restricted eigenvalues, Sparsity, Variable selection},
	pages = {1184--1226},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\NVZLS22R\\Hebiri and Geer - 2011 - The Smooth-Lasso and other ℓ1+ℓ2-penalized methods.pdf:application/pdf},
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634},
	doi = {10.1080/00401706.1970.10488634},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2025-12-16},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	month = feb,
	year = {1970},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634},
	pages = {55--67},
}

@article{hubert_comparing_1985,
	title = {Comparing partitions},
	volume = {2},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/BF01908075},
	doi = {10.1007/BF01908075},
	abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1.},
	language = {en},
	number = {1},
	urldate = {2025-12-16},
	journal = {Journal of Classification},
	author = {Hubert, Lawrence and Arabie, Phipps},
	month = dec,
	year = {1985},
	keywords = {Consensus indices, Measures of agreement, Measures of association},
	pages = {193--218},
}

@article{jolliffe_modified_2003,
	title = {A {Modified} {Principal} {Component} {Technique} {Based} on the {LASSO}},
	volume = {12},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/1061860032148},
	doi = {10.1198/1061860032148},
	abstract = {In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more difficult aspects of these techniques is the interpretation of the linear functions, as these functions usually have nonzero coefficients on all p variables. A common approach is to effectively ignore (treat as zero) any coefficients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coefficients to a smaller number of possible values in the derivation of the linear functions, or replace the principal components by “principal variables.” This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the “least absolute shrinkage and selection operator,” in which a bound is introduced on the sum of the absolute values of the coefficients, and in which some coefficients consequently become zero. We explore some of the properties of the new technique, both theoretically and using simulation studies, and apply it to an example.},
	number = {3},
	urldate = {2025-12-16},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Jolliffe, Ian T and Trendafilov, Nickolay T and Uddin, Mudassir},
	month = sep,
	year = {2003},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/1061860032148},
	keywords = {Interpretation, Principal component analysis, Simplification},
	pages = {531--547},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\VVFWIWA6\\Jolliffe et al. - 2003 - A Modified Principal Component Technique Based on the LASSO.pdf:application/pdf},
}

@book{kaufman_finding_2009,
	title = {Finding {Groups} in {Data}: {An} {Introduction} to {Cluster} {Analysis}},
	isbn = {978-0-470-31748-8},
	shorttitle = {Finding {Groups} in {Data}},
	abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "Cluster analysis is the increasingly important and practical subject of finding groupings in data. The authors set out to write a book for the user who does not necessarily have an extensive background in mathematics. They succeed very well."—Mathematical Reviews "Finding Groups in Data [is] a clear, readable, and interesting presentation of a small number of clustering methods. In addition, the book introduced some interesting innovations of applied value to clustering literature."—Journal of Classification "This is a very good, easy-to-read, and practical book. It has many nice features and is highly recommended for students and practitioners in various fields of study."—Technometrics An introduction to the practical application of cluster analysis, this text presents a selection of methods that together can deal with most applications. These methods are chosen for their robustness, consistency, and general applicability. This book discusses various types of data, including interval-scaled and binary variables as well as similarity data, and explains how these can be transformed prior to clustering.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Kaufman, Leonard and Rousseeuw, Peter J.},
	month = sep,
	year = {2009},
	note = {Google-Books-ID: YeFQHiikNo0C},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{macnaughton-smith_dissimilarity_1964,
	title = {Dissimilarity {Analysis}: a new {Technique} of {Hierarchical} {Sub}-division},
	volume = {202},
	copyright = {1964 Springer Nature Limited},
	issn = {1476-4687},
	shorttitle = {Dissimilarity {Analysis}},
	url = {https://www.nature.com/articles/2021034a0},
	doi = {10.1038/2021034a0},
	abstract = {IN hierarchical classifications each sub-group may be formed from the splitting into two parts of a larger group, or alternatively from the union of two smaller groups. Both these procedures are repetitive, and in either case ‘false’ decisions (arising from the statistical variability of the data) made in the early stages of the analysis will distort its subsequent course. For this reason, divisive methods, which start with the whole sample, are in general safer than agglomerative methods. In the past, one attraction of agglomerative methods has been their flexibility; any two sub-groups could be considered for possible combination. With divisive methods, in all but the simplest cases some restriction is necessary on the possible subdivisions considered, since there are 2n−1 − 1 ways of dividing n individuals into two groups. One procedure1 is to consider only monothetic subdivisions, that is, those definable in terms of the possession or lack of a single attribute by the individuals concerned.},
	language = {en},
	number = {4936},
	urldate = {2025-12-16},
	journal = {Nature},
	author = {Macnaughton-Smith, P. and Williams, W. T. and Dale, M. B. and Mockett, L. G.},
	month = jun,
	year = {1964},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {1034--1035},
}

@article{marcenko_distribution_1967,
	title = {{DISTRIBUTION} {OF} {EIGENVALUES} {FOR} {SOME} {SETS} {OF} {RANDOM} {MATRICES}},
	volume = {1},
	issn = {0025-5734},
	url = {https://www.mathnet.ru/eng/sm4101},
	doi = {10.1070/SM1967v001n04ABEH001994},
	language = {en},
	number = {4},
	urldate = {2025-12-16},
	journal = {Mathematics of the USSR-Sbornik},
	author = {Marčenko, V A and Pastur, L A},
	month = apr,
	year = {1967},
	pages = {457--483},
	file = {PDF:C\:\\Users\\nicsa\\Zotero\\storage\\CEQCPGN5\\Marčenko and Pastur - 1967 - DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES.pdf:application/pdf},
}

@article{moslehian_ky_2012,
	title = {Ky {Fan} inequalities},
	volume = {60},
	issn = {0308-1087},
	url = {https://doi.org/10.1080/03081087.2011.641545},
	doi = {10.1080/03081087.2011.641545},
	abstract = {There are several inequalities in the literature carrying the name of Ky Fan. We survey these well-known Ky Fan inequalities and some other significant inequalities generalized by Ky Fan and review some of their recent developments.},
	number = {11-12},
	urldate = {2025-12-16},
	journal = {Linear and Multilinear Algebra},
	author = {Moslehian, Mohammad Sal},
	month = nov,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03081087.2011.641545},
	keywords = {eigenvalue inequality, Kantorovich inequality, Ky Fan inequalities, Ky Fan norm, Ky Fan–Taussky–Todd inequality, Ky Fan–Todd determinantal inequality, M-matrix, matrix inequality, minimax inequality, Primary 00-02, Secondary 26D15, 47A63, 15A42, 15A45, 49J35, Szász's inequality},
	pages = {1313--1325},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\KHHRQI9U\\Moslehian - 2012 - Ky Fan inequalities.pdf:application/pdf},
}

@article{paul_asymptotics_nodate,
	title = {Asymptotics of the leading sample eigenvalues for a spiked covariance model},
	abstract = {We consider a multivariate Gaussian observation model where the covariance matrix is diagonal and the diagonal entries are all equal to one except for a ﬁnite number which are bigger. We address the question of asymptotic behaviour of the eigenvalues of the sample covariance matrix when the sample size and the dimension of the observations both grow to inﬁnity in such a way that their ratio converges to a positive constant. We establish almost sure limits of the largest few sample eigenvalues. We also show that when a population eigenvalue is above a certain threshold and of multiplicity one, the corresponding sample eigenvalue has a Gaussian limiting distribution. We also demonstrate a phase transition phenomenon of the sample eigenvectors in the same setting.},
	language = {en},
	author = {Paul, Debashis},
	file = {PDF:C\:\\Users\\nicsa\\Zotero\\storage\\X9UDH58Y\\Paul - Asymptotics of the leading sample eigenvalues for a spiked covariance model.pdf:application/pdf},
}

@article{paul_asymptotics_2007,
	title = {Asymptotics of {Sample} {Eigenstructure} for a {Large} {Dimensional} {Spiked} {Covariance} {Model}},
	volume = {17},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/24307692},
	abstract = {This paper deals with a multivariate Gaussian observation model where the eigenvalues of the covariance matrix are all one, except for a finite number which are larger. Of interest is the asymptotic behavior of the eigenvalues of the sample covariance matrix when the sample size and the dimension of the observations both grow to infinity so that their ratio converges to a positive constant. When a population eigenvalue is above a certain threshold and of multiplicity one, the corresponding sample eigenvalue has a Gaussian limiting distribution. There is a "phase transition" of the sample eigenvectors in the same setting. Another contribution here is a study of the second order asymptotics of sample eigenvectors when corresponding eigenvalues are simple and sufficiently large.},
	number = {4},
	urldate = {2025-12-16},
	journal = {Statistica Sinica},
	author = {Paul, Debashis},
	year = {2007},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {1617--1642},
	file = {JSTOR Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\G44BC277\\Paul - 2007 - Asymptotics of Sample Eigenstructure for a Large Dimensional Spiked Covariance Model.pdf:application/pdf},
}

@article{frs_liii_1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	url = {https://www.tandfonline.com/doi/pdf/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	abstract = {(1901). LIII. On lines and planes of closest fit to systems of points in space . The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 2, No. 11, pp. 559-572.},
	language = {EN},
	urldate = {2025-12-16},
	author = {F.R.S, Karl Pearson},
	month = nov,
	year = {1901},
	note = {Publisher: Taylor \& Francis Group},
	file = {Snapshot:C\:\\Users\\nicsa\\Zotero\\storage\\WILT885S\\14786440109462720.html:text/html},
}

@article{rand_objective_1971,
	title = {Objective {Criteria} for the {Evaluation} of {Clustering} {Methods}},
	volume = {66},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356},
	doi = {10.1080/01621459.1971.10482356},
	abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
	number = {336},
	urldate = {2025-12-16},
	journal = {Journal of the American Statistical Association},
	author = {Rand, William M.},
	month = dec,
	year = {1971},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482356},
	pages = {846--850},
}

@article{sjostrand_sparse_2007,
	title = {Sparse {Decomposition} and {Modeling} of {Anatomical} {Shape} {Variation}},
	volume = {26},
	issn = {1558-254X},
	url = {https://ieeexplore.ieee.org/abstract/document/4384468},
	doi = {10.1109/TMI.2007.898808},
	abstract = {Recent advances in statistics have spawned powerful methods for regression and data decomposition that promote sparsity, a property that facilitates interpretation of the results. Sparse models use a small subset of the available variables and may perform as well or better than their full counterparts if constructed carefully. In most medical applications, models are required to have both good statistical performance and a relevant clinical interpretation to be of value. Morphometry of the corpus callosum is one illustrative example. This paper presents a method for relating spatial features to clinical outcome data. A set of parsimonious variables is extracted using sparse principal component analysis, producing simple yet characteristic features. The relation of these variables with clinical data is then established using a regression model. The result may be visualized as patterns of anatomical variation related to clinical outcome. In the present application, landmark-based shape data of the corpus callosum is analyzed in relation to age, gender, and clinical tests of walking speed and verbal fluency. To put the data-driven sparse principal component method into perspective, we consider two alternative techniques, one where features are derived using a model-based wavelet approach, and one where the original variables are regressed directly on the outcome.},
	number = {12},
	urldate = {2025-12-16},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Sjostrand, Karl and Rostrup, Egill and Ryberg, Charlotte and Larsen, Rasmus and Studholme, Colin and Baezner, Hansjoerg and Ferro, Jose and Fazekas, Franz and Pantoni, Leonardo and Inzitari, Domenico and Waldemar, Gunhild},
	month = dec,
	year = {2007},
	keywords = {Anatomy, Biomedical imaging, Corpus callosum (CC), Data mining, decomposition, Hospitals, Informatics, Leukoaraiosis And DISability in the elderly (LADIS), Magnetic resonance, Mathematical model, Nervous system, Principal component analysis, principal component analysis (PCA), Shape, shape analysis, sparse},
	pages = {1625--1635},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\T3TXDDZ2\\Sjostrand et al. - 2007 - Sparse Decomposition and Modeling of Anatomical Shape Variation.pdf:application/pdf},
}

@book{tao_topics_2023,
	title = {Topics in {Random} {Matrix} {Theory}},
	isbn = {978-1-4704-7459-1},
	abstract = {The field of random matrix theory has seen an explosion of activity in recent years, with connections to many areas of mathematics and physics. However, this makes the current state of the field almost too large to survey in a single book. In this graduate text, we focus on one specific sector of the field, namely the spectral distribution of random Wigner matrix ensembles (such as the Gaussian Unitary Ensemble), as well as iid matrix ensembles. The text is largely self-contained and starts with a review of relevant aspects of probability theory and linear algebra. With over 200 exercises, the book is suitable as an introductory text for beginning graduate students seeking to enter the field.},
	language = {en},
	publisher = {American Mathematical Society},
	author = {Tao, Terence},
	month = aug,
	year = {2023},
	note = {Google-Books-ID: AwDUEAAAQBAJ},
	keywords = {Mathematics / General},
}

@misc{noauthor_regression_nodate,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso} {\textbar} {Journal} of the {Royal} {Statistical} {Society} {Series} {B}: {Statistical} {Methodology} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/jrsssb/article/58/1/267/7027929},
	urldate = {2025-12-16},
}

@misc{noauthor_estimating_nodate,
	title = {Estimating the number of clusters in a data set via the gap statistic - {Tibshirani} - 2001 - {Journal} of the {Royal} {Statistical} {Society}: {Series} {B} ({Statistical} {Methodology}) - {Wiley} {Online} {Library}},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00293?casa_token=oX3_DGks190AAAAA:Gncp5Q2oZJAD5rb4z2t4xNEMAYQyXERwifgZI7vSkSaiWvgeKKPfgqlfZXU4EMq-r4BOLXfxlzufs5w},
	urldate = {2025-12-16},
	file = {Estimating the number of clusters in a data set via the gap statistic - Tibshirani - 2001 - Journal of the Royal Statistical Society\: Series B (Statistical Methodology) - Wiley Online Library:C\:\\Users\\nicsa\\Zotero\\storage\\DI4DHBMK\\1467-9868.html:text/html},
}

@article{yeung_empirical_2001,
	title = {An empirical study on {Principal} {Component} {Analysis} for clustering gene expression data},
	abstract = {There is a great need to develop analytical methodology to analyze and to exploit the information contained in gene expression data. Because of the large number of genes and the complexity of biological networks, clustering is a useful exploratory technique for analysis of gene expression data. Other classical techniques, such as principal component analysis (PCA), have also been applied to analyze gene expression data. Using different data analysis techniques and different clustering algorithms to analyze the same data set can lead to very different conclusions. Our goal is to study the effectiveness of principal components (PC’s) in capturing cluster structure. In other words, we empirically compared the quality of clusters obtained from the original data set to the quality of clusters obtained from clustering the PC’s using both real gene expression data sets and synthetic data sets.},
	language = {en},
	author = {Yeung, Ka Yee and Ruzzo, Walter L},
	year = {2001},
	file = {PDF:C\:\\Users\\nicsa\\Zotero\\storage\\7C4YNYY8\\Yeung and Ruzzo - An empirical study on Principal Component Analysis for clustering gene expression data.pdf:application/pdf},
}

@article{zou_sparse_2006,
	title = {Sparse {Principal} {Component} {Analysis}},
	volume = {15},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186006X113430},
	doi = {10.1198/106186006X113430},
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
	number = {2},
	urldate = {2025-12-16},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186006X113430},
	keywords = {Arrays, Gene expression, Lasso/elastic net, Multivariate analysis, Singular value decomposition, Thresholding},
	pages = {265--286},
	file = {Full Text PDF:C\:\\Users\\nicsa\\Zotero\\storage\\D9AUY6R6\\Zou et al. - 2006 - Sparse Principal Component Analysis.pdf:application/pdf},
}

@misc{mukherjee_compressibility_2022,
	title = {Compressibility: {Power} of {PCA} in {Clustering} {Problems} {Beyond} {Dimensionality} {Reduction}},
	shorttitle = {Compressibility},
	url = {https://www.researchgate.net/publication/360186068_Compressibility_Power_of_PCA_in_Clustering_Problems_Beyond_Dimensionality_Reduction},
	abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
	language = {en},
	urldate = {2025-12-16},
	journal = {ResearchGate},
	author = {Mukherjee, Chandra and Zhang, Jiapeng},
	year = {2022},
	file = {Snapshot:C\:\\Users\\nicsa\\Zotero\\storage\\8FF5GIYN\\360186068_Compressibility_Power_of_PCA_in_Clustering_Problems_Beyond_Dimensionality_Reduction.html:text/html},
}
